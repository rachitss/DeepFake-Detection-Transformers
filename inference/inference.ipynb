{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b2088ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a519736",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = r\"D:\\W\\VS\\VS Folder\\DFD\\DFD-T\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc7f95d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         filename  label  split        original  \\\n",
      "0  aagfhgtpmv.mp4      1  train  vudstovrck.mp4   \n",
      "1  aapnvogymq.mp4      1  train  jdubbvfswz.mp4   \n",
      "2  abarnvbtwb.mp4      0  train             NaN   \n",
      "3  abqwwspghj.mp4      1  train  qzimuostzz.mp4   \n",
      "4  acifjvzvpm.mp4      1  train  kbvibjhfzo.mp4   \n",
      "\n",
      "                                                path  \n",
      "0  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aag...  \n",
      "1  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aap...  \n",
      "2  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aba...  \n",
      "3  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\abq...  \n",
      "4  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aci...  \n"
     ]
    }
   ],
   "source": [
    "valid_train_df = pd.read_csv(rf\"{base}\\valid_train_df.csv\")\n",
    "# Add index to DataFrame\n",
    "print(valid_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3083ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analysis of values\n",
    "\n",
    "# label_counts = valid_train_df['label'].value_counts()\n",
    "# label_0_count = label_counts.get(0, 0)\n",
    "# label_1_count = label_counts.get(1, 0)\n",
    "# print(f\"Label 0 count: {label_0_count}, Label 1 count: {label_1_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b36b684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\cizlkenljw , 0\n"
     ]
    }
   ],
   "source": [
    "row=valid_train_df.iloc[192]\n",
    "print(row['path'],',', row['label'])\n",
    "path= row['path']\n",
    "label = row['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07fbb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = rf'{base}\\swin_tiny.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d95aae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and processor\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", num_labels=1, ignore_mismatched_sizes=True)\n",
    "# classifier = nn.Linear(768, 1)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')  # For binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee3b0a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "# classifier.load_state_dict(torch.load('vit_classifier_head.pth'))\n",
    "model = model.to(device)\n",
    "# classifier = classifier.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a8331a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbeb8f21a58a42df81b3bc0037ba56c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Predicted class: 0, Actual class: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 36\n",
    "pred_classes = []\n",
    "total_batches = 0\n",
    "\n",
    "losses = []\n",
    "probabs = []\n",
    "correct = 0\n",
    "total = 0\n",
    "total_batches = 0\n",
    "\n",
    "image_files = sorted([ os.path.join(path, f)\n",
    "for f in os.listdir(path)\n",
    "if f.lower().endswith('.png')\n",
    "])\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(image_files), batch_size), desc='Image Batches', leave=False):\n",
    "    total_batches += 1\n",
    "    batch_paths = image_files[i:i+batch_size]\n",
    "    images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "    inputs = processor(images=images, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    batch_labels = torch.tensor([label] * len(images), dtype=torch.float).unsqueeze(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        batch_logits = outputs.logits\n",
    "\n",
    "        # running_loss += criterion(batch_logits, batch_labels).item()* batch_labels.size(0)\n",
    "        loss = criterion(batch_logits, batch_labels).detach().cpu().view(-1).tolist()\n",
    "        losses.extend(loss)\n",
    "\n",
    "        prob=torch.sigmoid(batch_logits).detach().cpu().view(-1).tolist()\n",
    "        probabs.extend(prob)\n",
    "\n",
    "        pred = (torch.sigmoid(batch_logits) > 0.5).int()\n",
    "        correct += (pred == batch_labels.int()).sum().item()\n",
    "        total += batch_labels.size(0)\n",
    "        pred_classes.extend(pred.cpu().numpy().flatten().tolist())\n",
    "        \n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "val_loss = sum(losses) / total if total > 0 else 0\n",
    "\n",
    "\n",
    "pred = int(sum(pred_classes) > len(pred_classes) / 2)\n",
    "display(f\"Predicted class: {pred}, Actual class: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD CODE IGNORE\n",
    "\n",
    "\n",
    "# batch_size = 36\n",
    "# pred_classes = []\n",
    "# total_batches = 0\n",
    "\n",
    "# image_files = sorted([ os.path.join(path, f)\n",
    "# for f in os.listdir(path)\n",
    "# if f.lower().endswith('.png')\n",
    "# ])\n",
    "\n",
    "# for i in tqdm(range(0, len(image_files), batch_size), desc='Image Batches', leave=False):\n",
    "#     total_batches += 1\n",
    "#     batch_paths = image_files[i:i+batch_size]\n",
    "#     images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "#     inputs = processor(images=images, return_tensors=\"pt\")\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#         cls_embeddings = outputs.last_hidden_state[:, 0, :]  # (batch, hidden_dim)\n",
    "#         outputs = classifier(cls_embeddings)  # (batch, 1)\n",
    "#         preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "#         pred_classes.extend(preds.cpu().numpy().flatten().tolist())\n",
    "\n",
    "# pred = int(sum(pred_classes) > len(pred_classes) / 2)\n",
    "# display(f\"Predicted class: {pred}, Actual class: {label}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
