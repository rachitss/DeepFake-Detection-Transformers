{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db38f9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\W\\VS\\VS Folder\\DFD\\env1\\Lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision.models import resnet18\n",
    "from albumentations import Normalize, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Running on device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a0cc51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from upload_video import get_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9338e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drag_drop_video.py\n",
    "# from tkinter import Tk, Label\n",
    "# from tkinterdnd2 import DND_FILES, TkinterDnD\n",
    "# from pathlib import Path\n",
    "\n",
    "# def handle_drop(event):\n",
    "#     raw = event.data\n",
    "#     # Windows provides full paths in braces like {C:\\path\\video.mp4}\n",
    "#     # macOS/Linux give plain paths; this cleans both\n",
    "#     video_path = os.path.normpath(raw).replace(\"\\\\\", \"/\")\n",
    "#     print(\"User provided video:\", video_path)\n",
    "\n",
    "#     # now run your MTCNN pipeline\n",
    "#     # run_mtcnn(video_path)\n",
    "\n",
    "# root = TkinterDnD.Tk()\n",
    "# root.title(\"Drop a Video File\")\n",
    "\n",
    "# label = Label(root, text=\"Drag and drop a video file here\", width=40, height=10)\n",
    "# label.pack(padx=20, pady=20)\n",
    "\n",
    "# label.drop_target_register(DND_FILES)\n",
    "# label.dnd_bind(\"<<Drop>>\", handle_drop)\n",
    "\n",
    "# root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "VID_PATH = get_video()\n",
    "TMP_DIR = 'MTCNN EXTRACTED/'\n",
    "\n",
    "# os.makedirs(VID_PATH, exist_ok=True)\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "SCALE = 0.25\n",
    "N_FRAMES = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a405c",
   "metadata": {},
   "source": [
    "### Face Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd1b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceExtractor:\n",
    "    def __init__(self, detector, n_frames=None, resize=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            n_frames {int} -- Total number of frames to load. These will be evenly spaced\n",
    "                throughout the video. If not specified (i.e., None), all frames will be loaded.\n",
    "                (default: {None})\n",
    "            resize {float} -- Fraction by which to resize frames from original prior to face\n",
    "                detection. A value less than 1 results in downsampling and a value greater than\n",
    "                1 result in upsampling. (default: {None})\n",
    "        \"\"\"\n",
    "\n",
    "        self.detector = detector\n",
    "        self.n_frames = n_frames\n",
    "        self.resize = resize\n",
    "    \n",
    "    def __call__(self, filename, save_dir):\n",
    "        \"\"\"Load frames from an MP4 video, detect faces and save the results.\n",
    "\n",
    "        Parameters:\n",
    "            filename {str} -- Path to video.\n",
    "            save_dir {str} -- The directory where results are saved.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create video reader and find length\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Pick 'n_frames' evenly spaced frames to sample\n",
    "        if self.n_frames is None:\n",
    "            sample = np.arange(0, v_len)\n",
    "        else:\n",
    "            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\n",
    "\n",
    "        # Loop through frames\n",
    "        for j in range(v_len):\n",
    "            success = v_cap.grab()\n",
    "            if j in sample:\n",
    "                # Load frame\n",
    "                success, frame = v_cap.retrieve()\n",
    "                if not success:\n",
    "                    continue\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = Image.fromarray(frame)\n",
    "                \n",
    "                # Resize frame to desired size\n",
    "                if self.resize is not None:\n",
    "                    frame = frame.resize([int(d * self.resize) for d in frame.size])\n",
    "\n",
    "                save_path = os.path.join(save_dir, f'{j}.png')\n",
    "\n",
    "                self.detector([frame], save_path=save_path)\n",
    "\n",
    "        v_cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face detector\n",
    "face_detector = MTCNN(margin=14, keep_all=True, factor=0.5, device=device).eval()\n",
    "\n",
    "# Define face extractor\n",
    "face_extractor = FaceExtractor(detector=face_detector, n_frames=N_FRAMES, resize=SCALE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    file_name = VID_PATH.split('\\\\')[-1]\n",
    "\n",
    "    save_dir = os.path.join(TMP_DIR, file_name.split(\".\")[0])\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Detect all faces appear in the video and save them.\n",
    "    face_extractor(VID_PATH, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29649930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
