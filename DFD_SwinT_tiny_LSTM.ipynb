{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69efea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoImageProcessor, AutoModel  # ADDED [use AutoModel backbone]\n",
    "# from transformers import AutoModelForImageClassification  # COMMENTED OUT: replaced by AutoModel backbone\n",
    "from PIL import Image\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchviz import make_dot\n",
    "from ptflops import get_model_complexity_info\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Running on device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a4cda5",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204f32de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aagfhgtpmv.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>vudstovrck.mp4</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aapnvogymq.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>jdubbvfswz.mp4</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abarnvbtwb.mp4</td>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abofeumbvv.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>atvmxvwyns.mp4</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abqwwspghj.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>qzimuostzz.mp4</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\abq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>etejaapnxh.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>wtreibcmgm.mp4</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\ete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>etmcruaihe.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>afoovlsmtx.mp4</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\etm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>etohcvnzbj.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>bdnaqemxmr.mp4</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\eto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>eudeqjhdfd.mp4</td>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\eud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>eukvucdetx.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>gjypopglvi.mp4</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\euk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename label  split        original  \\\n",
       "0    aagfhgtpmv.mp4  FAKE  train  vudstovrck.mp4   \n",
       "1    aapnvogymq.mp4  FAKE  train  jdubbvfswz.mp4   \n",
       "2    abarnvbtwb.mp4  REAL  train             NaN   \n",
       "3    abofeumbvv.mp4  FAKE  train  atvmxvwyns.mp4   \n",
       "4    abqwwspghj.mp4  FAKE  train  qzimuostzz.mp4   \n",
       "..              ...   ...    ...             ...   \n",
       "395  etejaapnxh.mp4  FAKE  train  wtreibcmgm.mp4   \n",
       "396  etmcruaihe.mp4  FAKE  train  afoovlsmtx.mp4   \n",
       "397  etohcvnzbj.mp4  FAKE  train  bdnaqemxmr.mp4   \n",
       "398  eudeqjhdfd.mp4  REAL  train             NaN   \n",
       "399  eukvucdetx.mp4  FAKE  train  gjypopglvi.mp4   \n",
       "\n",
       "                                                  path  \n",
       "0    D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aag...  \n",
       "1    D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aap...  \n",
       "2    D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aba...  \n",
       "3    D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\abo...  \n",
       "4    D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\abq...  \n",
       "..                                                 ...  \n",
       "395  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\ete...  \n",
       "396  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\etm...  \n",
       "397  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\eto...  \n",
       "398  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\eud...  \n",
       "399  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\euk...  \n",
       "\n",
       "[400 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = 'D:\\\\W\\\\VS\\\\VS Folder\\\\DFD\\\\DFDC MTCNN Extracted\\\\'\n",
    "train_df = pd.read_csv(os.path.join(train_dir, 'metadata.csv'))\n",
    "train_df['path'] = train_df['filename'].apply(lambda x: os.path.join(train_dir, x.split('.')[0]))\n",
    "\n",
    "# Remove empty folders\n",
    "train_df = train_df[train_df['path'].map(lambda x: os.path.exists(x))]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23dbafc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9f2e8824fe45478e99ab6e617c7261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_train_df = pd.DataFrame(columns=['filename', 'label', 'split', 'original', 'path'])\n",
    "\n",
    "# for row_idx, row in tqdm(train_df.iterrows()):\n",
    "for row_idx in tqdm(train_df.index):\n",
    "    row = train_df.loc[row_idx]\n",
    "    img_dir = row['path']\n",
    "    face_paths = glob.glob(f'{img_dir}/*.png')\n",
    "\n",
    "    if len(face_paths) >= 5: # Satisfy the minimum requirement for the number of faces\n",
    "        face_indices = [\n",
    "            path.split('\\\\')[-1].split('.')[0].split('_')[0]\n",
    "            for path in face_paths\n",
    "        ]\n",
    "        max_idx = np.max(np.array(face_indices, dtype=np.uint32))\n",
    "\n",
    "        selected_paths = []\n",
    "\n",
    "        for i in range(5):\n",
    "            stride = int((max_idx + 1)/(5**2))\n",
    "            sample = np.linspace(i*stride, max_idx + i*stride, 5).astype(int)\n",
    "\n",
    "            # Get faces\n",
    "            for idx in sample:\n",
    "                paths = glob.glob(f'{img_dir}/{idx}*.png')\n",
    "\n",
    "                selected_paths.extend(paths)\n",
    "                if len(selected_paths) >= 5: # Get enough faces\n",
    "                    break\n",
    "\n",
    "            if len(selected_paths) >= 5:  # Get enough faces\n",
    "                valid_train_df = pd.concat([valid_train_df, pd.DataFrame([row])], ignore_index=True)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e89472c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rachi\\AppData\\Local\\Temp\\ipykernel_8608\\3380636837.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  valid_train_df['label']=valid_train_df['label'].replace({'FAKE': 1, 'REAL': 0})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aagfhgtpmv.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>vudstovrck.mp4</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aapnvogymq.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>jdubbvfswz.mp4</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abarnvbtwb.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abqwwspghj.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>qzimuostzz.mp4</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\abq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acifjvzvpm.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>kbvibjhfzo.mp4</td>\n",
       "      <td>D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename  label  split        original  \\\n",
       "0  aagfhgtpmv.mp4      1  train  vudstovrck.mp4   \n",
       "1  aapnvogymq.mp4      1  train  jdubbvfswz.mp4   \n",
       "2  abarnvbtwb.mp4      0  train             NaN   \n",
       "3  abqwwspghj.mp4      1  train  qzimuostzz.mp4   \n",
       "4  acifjvzvpm.mp4      1  train  kbvibjhfzo.mp4   \n",
       "\n",
       "                                                path  \n",
       "0  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aag...  \n",
       "1  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aap...  \n",
       "2  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aba...  \n",
       "3  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\abq...  \n",
       "4  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aci...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0     76\n",
      "1    306\n",
      "Name: filename, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "valid_train_df['label']=valid_train_df['label'].replace({'FAKE': 1, 'REAL': 0})\n",
    "display(valid_train_df.head())\n",
    "\n",
    "label_count = valid_train_df.groupby('label').count()['filename']\n",
    "print(label_count)\n",
    "\n",
    "x = valid_train_df['path'].to_numpy()\n",
    "y = valid_train_df['label'].to_numpy()\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.3, random_state=224, stratify=y)\n",
    "\n",
    "np.savez('train_test_split.npz',\n",
    "         x_train=x_train,\n",
    "         x_val=x_val,\n",
    "         y_train=y_train,\n",
    "         y_val=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd86ca",
   "metadata": {},
   "source": [
    "# Training - Feature extraction and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89cafb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('train_test_split.npz', allow_pickle=True)\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67fd378",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1967d768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "\n",
    "# model = AutoModelForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", num_labels=1, ignore_mismatched_sizes=True)  # COMMENTED OUT\n",
    "# ADDED: load Swin backbone to get hidden states (features) per frame\n",
    "model = AutoModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")  # ADDED\n",
    "hidden_size = model.config.hidden_size  # ADDED: feature size for LSTM input\n",
    "\n",
    "# ADDED: temporal head (BiLSTM + Linear -> 1 logit)\n",
    "temporal_lstm = nn.LSTM(\n",
    "    input_size=hidden_size,   # from Swin hidden size\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    batch_first=True,\n",
    "    bidirectional=True,\n",
    "    dropout=0.1).to(device)  # ADDED\n",
    "\n",
    "temporal_head = nn.Linear(512 * 2, 1).to(device)  # ADDED\n",
    "\n",
    "# classifier = nn.Linear(768, 1)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')  # For binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c65a592c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinModel(\n",
      "  (embeddings): SwinEmbeddings(\n",
      "    (patch_embeddings): SwinPatchEmbeddings(\n",
      "      (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "    )\n",
      "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): SwinEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): SwinStage(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinLayer(\n",
      "            (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): SwinAttention(\n",
      "              (self): SwinSelfAttention(\n",
      "                (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                (key): Linear(in_features=96, out_features=96, bias=True)\n",
      "                (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): SwinSelfOutput(\n",
      "                (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): SwinIntermediate(\n",
      "              (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): SwinOutput(\n",
      "              (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinLayer(\n",
      "            (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): SwinAttention(\n",
      "              (self): SwinSelfAttention(\n",
      "                (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                (key): Linear(in_features=96, out_features=96, bias=True)\n",
      "                (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): SwinSelfOutput(\n",
      "                (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): SwinDropPath(p=0.00909090880304575)\n",
      "            (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): SwinIntermediate(\n",
      "              (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): SwinOutput(\n",
      "              (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): SwinPatchMerging(\n",
      "          (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): SwinStage(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinLayer(\n",
      "            (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): SwinAttention(\n",
      "              (self): SwinSelfAttention(\n",
      "                (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): SwinSelfOutput(\n",
      "                (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): SwinDropPath(p=0.0181818176060915)\n",
      "            (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): SwinIntermediate(\n",
      "              (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): SwinOutput(\n",
      "              (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinLayer(\n",
      "            (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): SwinAttention(\n",
      "              (self): SwinSelfAttention(\n",
      "                (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): SwinSelfOutput(\n",
      "                (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): SwinDropPath(p=0.027272727340459824)\n",
      "            (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): SwinIntermediate(\n",
      "              (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): SwinOutput(\n",
      "              (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): SwinPatchMerging(\n",
      "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): SwinStage(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinLayer(\n",
      "            (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): SwinAttention(\n",
      "              (self): SwinSelfAttention(\n",
      "                (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): SwinSelfOutput(\n",
      "                (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): SwinDropPath(p=0.036363635212183)\n",
      "            (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): SwinIntermediate(\n",
      "              (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): SwinOutput(\n",
      "              (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinLayer(\n",
      "            (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): SwinAttention(\n",
      "              (self): SwinSelfAttention(\n",
      "                (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): SwinSelfOutput(\n",
      "                (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): SwinDropPath(p=0.045454543083906174)\n",
      "            (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): SwinIntermediate(\n",
      "              (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): SwinOutput(\n",
      "              (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinLayer(\n",
      "            (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): SwinAttention(\n",
      "              (self): SwinSelfAttention(\n",
      "                (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): SwinSelfOutput(\n",
      "                (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): SwinDropPath(p=0.054545458406209946)\n",
      "            (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): SwinIntermediate(\n",
      "              (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): SwinOutput(\n",
      "              (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinLayer(\n",
      "            (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): SwinAttention(\n",
      "              (self): SwinSelfAttention(\n",
      "                (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): SwinSelfOutput(\n",
      "                (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): SwinDropPath(p=0.06363636255264282)\n",
      "            (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): SwinIntermediate(\n",
      "              (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): SwinOutput(\n",
      "              (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinLayer(\n",
      "            (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): SwinAttention(\n",
      "              (self): SwinSelfAttention(\n",
      "                (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): SwinSelfOutput(\n",
      "                (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): SwinDropPath(p=0.0727272778749466)\n",
      "            (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): SwinIntermediate(\n",
      "              (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): SwinOutput(\n",
      "              (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinLayer(\n",
      "            (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): SwinAttention(\n",
      "              (self): SwinSelfAttention(\n",
      "                (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): SwinSelfOutput(\n",
      "                (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): SwinDropPath(p=0.08181818574666977)\n",
      "            (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): SwinIntermediate(\n",
      "              (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): SwinOutput(\n",
      "              (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): SwinPatchMerging(\n",
      "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): SwinStage(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinLayer(\n",
      "            (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): SwinAttention(\n",
      "              (self): SwinSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): SwinSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): SwinDropPath(p=0.09090909361839294)\n",
      "            (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): SwinIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): SwinOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinLayer(\n",
      "            (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): SwinAttention(\n",
      "              (self): SwinSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): SwinSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): SwinDropPath(p=0.10000000149011612)\n",
      "            (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): SwinIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): SwinOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (pooler): AdaptiveAvgPool1d(output_size=1)\n",
      ")\n",
      "LSTM(768, 512, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "Linear(in_features=1024, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(temporal_lstm)\n",
    "print(temporal_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc2a9ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 total layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Unfreezing layer: SwinStage(\\n  (blocks): ModuleList(\\n    (0): SwinLayer(\\n      (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n      (attention): SwinAttention(\\n        (self): SwinSelfAttention(\\n          (query): Linear(in_features=768, out_features=768, bias=True)\\n          (key): Linear(in_features=768, out_features=768, bias=True)\\n          (value): Linear(in_features=768, out_features=768, bias=True)\\n          (dropout): Dropout(p=0.0, inplace=False)\\n        )\\n        (output): SwinSelfOutput(\\n          (dense): Linear(in_features=768, out_features=768, bias=True)\\n          (dropout): Dropout(p=0.0, inplace=False)\\n        )\\n      )\\n      (drop_path): SwinDropPath(p=0.09090909361839294)\\n      (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n      (intermediate): SwinIntermediate(\\n        (dense): Linear(in_features=768, out_features=3072, bias=True)\\n        (intermediate_act_fn): GELUActivation()\\n      )\\n      (output): SwinOutput(\\n        (dense): Linear(in_features=3072, out_features=768, bias=True)\\n        (dropout): Dropout(p=0.0, inplace=False)\\n      )\\n    )\\n    (1): SwinLayer(\\n      (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n      (attention): SwinAttention(\\n        (self): SwinSelfAttention(\\n          (query): Linear(in_features=768, out_features=768, bias=True)\\n          (key): Linear(in_features=768, out_features=768, bias=True)\\n          (value): Linear(in_features=768, out_features=768, bias=True)\\n          (dropout): Dropout(p=0.0, inplace=False)\\n        )\\n        (output): SwinSelfOutput(\\n          (dense): Linear(in_features=768, out_features=768, bias=True)\\n          (dropout): Dropout(p=0.0, inplace=False)\\n        )\\n      )\\n      (drop_path): SwinDropPath(p=0.10000000149011612)\\n      (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n      (intermediate): SwinIntermediate(\\n        (dense): Linear(in_features=768, out_features=3072, bias=True)\\n        (intermediate_act_fn): GELUActivation()\\n      )\\n      (output): SwinOutput(\\n        (dense): Linear(in_features=3072, out_features=768, bias=True)\\n        (dropout): Dropout(p=0.0, inplace=False)\\n      )\\n    )\\n  )\\n)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "temporal_lstm.train()   # ADDED\n",
    "temporal_head.train()   # ADDED\n",
    "\n",
    "# freeze model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(len(model.encoder.layers),\"total layers\")\n",
    "\n",
    "# unfreeze n layer parameters\n",
    "n = 1\n",
    "for layer in model.encoder.layers[-n:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "    display(f'Unfreezing layer: {layer}')\n",
    "\n",
    "# for param in model.classifier.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# UPDATED: optimize unfrozen Swin encoder params + temporal head params (LSTM + head)\n",
    "optimizer = torch.optim.Adam(\n",
    "    # list(model.classifier.parameters()) +   # COMMENTED OUT\n",
    "    [p for p in model.encoder.parameters() if p.requires_grad]  # keep unfrozen encoder params\n",
    "    + list(temporal_lstm.parameters())  # ADDED\n",
    "    + list(temporal_head.parameters()), # ADDED\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e2c97",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b69100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights not found, starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58eca49ba1ca4e9bb27ed2605fefeac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4624762bc84d1f8cebdd37b47dc108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Videos:   0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(image_files), batch_size):\n\u001b[32m     64\u001b[39m     batch_paths = image_files[i:i+batch_size]\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     images = [\u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m batch_paths]\n\u001b[32m     66\u001b[39m     inputs = processor(images=images, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m     inputs = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\W\\VS\\VS Folder\\DFD\\env1\\Lib\\site-packages\\PIL\\Image.py:3469\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3466\u001b[39m     filename = os.path.realpath(os.fspath(fp))\n\u001b[32m   3468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[32m-> \u001b[39m\u001b[32m3469\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3470\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3471\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if os.path.exists('swin_tiny_lstm.pth'):\n",
    "    print(\"Loading existing model weights...\")\n",
    "    state = torch.load('swin_tiny_lstm.pth', map_location='cpu')\n",
    "    # UPDATED: handle both old single-module checkpoints and new dict checkpoints\n",
    "    \n",
    "\n",
    "    model.load_state_dict(state['backbone'], strict=False)        # UPDATED\n",
    "    temporal_lstm.load_state_dict(state['temporal_lstm'])         # ADDED\n",
    "    temporal_head.load_state_dict(state['temporal_head'])         # ADDED\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "    model = model.to(device)\n",
    "    temporal_lstm = temporal_lstm.to(device)                           # ADDED\n",
    "    temporal_head = temporal_head.to(device)                           # ADDED\n",
    "    \n",
    "    print(\"Resuming training with loaded model weights.\")\n",
    "else:\n",
    "    print(\"Model weights not found, starting training...\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "batch_size = 16\n",
    "labels = []\n",
    "video_paths = x_train\n",
    "video_labels = y_train\n",
    "total_batches = 0\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 20  # Adjust as needed\n",
    "\n",
    "train_loss = np.zeros(num_epochs)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "epoch_bar = tqdm(range(num_epochs), desc='Epochs')\n",
    "for epoch in epoch_bar:\n",
    "    model.train()\n",
    "    # classifier.train()\n",
    "    temporal_lstm.train()   # ADDED\n",
    "    temporal_head.train()   # ADDED\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    videos_processed = 0  # ADDED\n",
    "\n",
    "    video_bar = tqdm(zip(video_paths, video_labels),desc='Videos', total=len(video_paths))\n",
    "    for path, label in video_bar:\n",
    "        image_files = sorted([ os.path.join(path, f)\n",
    "        for f in os.listdir(path)\n",
    "        if f.lower().endswith('.png')\n",
    "        ])\n",
    "\n",
    "        labels.extend([label] * len(image_files))  # Repeat label for each image\n",
    "\n",
    "        video_run_loss=0.0\n",
    "        batches = 0\n",
    "\n",
    "        # ADDED: collect per-frame embeddings for the whole video\n",
    "        frame_feats = []  # ADDED\n",
    "\n",
    "        for i in range(0, len(image_files), batch_size):\n",
    "            batch_paths = image_files[i:i+batch_size]\n",
    "            images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "            inputs = processor(images=images, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            outputs = model(**inputs, output_hidden_states=False, return_dict=True)  # ADDED\n",
    "            \n",
    "            last_hidden = outputs.last_hidden_state  # (B, S, C)  # ADDED\n",
    "            pooled = last_hidden.mean(dim=1)         # (B, C)     # ADDED\n",
    "            frame_feats.append(pooled)  # ADDED\n",
    "\n",
    "            batches += 1\n",
    "            total_batches += 1\n",
    "\n",
    "        # ADDED: assemble sequence and run LSTM head to get a single logit per video\n",
    "\n",
    "        if len(frame_feats) > 0:\n",
    "            seq = torch.cat(frame_feats, dim=0)               # (T, C)     # ADDED\n",
    "            seq = seq.unsqueeze(0)                            # (1, T, C)  # ADDED\n",
    "\n",
    "            optimizer.zero_grad()                             # ADDED\n",
    "            lstm_out, _ = temporal_lstm(seq)                  # (1, T, H*2) # ADDED\n",
    "            last = lstm_out[:, -1, :]                         # (1, H*2)    # ADDED\n",
    "            video_logit = temporal_head(last)                 # (1, 1)      # ADDED\n",
    "\n",
    "            video_label = torch.tensor([[label]], dtype=torch.float, device=device)  # (1,1)  # ADDED\n",
    "            loss = criterion(video_logit, video_label)        # (1,1) with reduction='none'   # ADDED\n",
    "            loss = loss.mean()                                # scalar for backward            # ADDED\n",
    "            loss.backward()                                   # ADDED\n",
    "            optimizer.step()                                  # ADDED\n",
    "\n",
    "            loss_item = loss.item()                           # ADDED\n",
    "            running_loss += loss_item                         # ADDED\n",
    "            video_run_loss += loss_item                       # ADDED\n",
    "\n",
    "            videos_processed += 1                              # ADDED\n",
    "\n",
    "        video_loss = video_run_loss\n",
    "        video_bar.set_postfix(Last_Loss=f'{video_loss:.4f}')\n",
    "\n",
    "    avg_loss = running_loss / videos_processed if videos_processed > 0 else 0  # UPDATED\n",
    "    train_loss[epoch] = avg_loss\n",
    "    epoch_bar.set_postfix(Curr_Loss=f'{avg_loss:.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Save model backbone and temporal head\n",
    "torch.save({\n",
    "    'backbone': model.state_dict(),          # UPDATED\n",
    "    'temporal_lstm': temporal_lstm.state_dict(),  # ADDED\n",
    "    'temporal_head': temporal_head.state_dict(),   # ADDED\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}, 'swin_tiny_lstm.pth')\n",
    "\n",
    "# Saving training loss\n",
    "\n",
    "if os.path.exists('swin_tiny_lstm_train_loss.npy'):\n",
    "    # loading existing training loss\n",
    "    training_loss = np.load('swin_tiny_lstm_train_loss.npy')\n",
    "    training_loss = np.concatenate((training_loss, train_loss))\n",
    "    np.save('swin_tiny_lstm_train_loss.npy', training_loss)\n",
    "\n",
    "else:\n",
    "    training_loss = train_loss\n",
    "    np.save('swin_tiny_lstm_train_loss.npy', training_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36a7579b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3, 224, 224])\n",
      "Training completed in 261.07 minutes\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"pixel_values\"].shape)\n",
    "print(f\"Training completed in {training_time/60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bd15f",
   "metadata": {},
   "source": [
    "Log Loss Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea5f2f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m epochs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtraining_loss\u001b[49m) + \u001b[32m1\u001b[39m))\n\u001b[32m      2\u001b[39m plt.figure(figsize=(\u001b[32m16\u001b[39m, \u001b[32m8\u001b[39m))\n\u001b[32m      3\u001b[39m plt.plot(epochs, training_loss)\n",
      "\u001b[31mNameError\u001b[39m: name 'training_loss' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = list(range(1, len(training_loss) + 1))\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(epochs, training_loss)\n",
    "plt.title('Training Log Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Log Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879e9e48",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8072c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('swin_tiny_lstm.pth'))\n",
    "# classifier.load_state_dict(torch.load('vit_classifier_head.pth'))\n",
    "model = model.to(device)\n",
    "# classifier = classifier.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303bcfe",
   "metadata": {},
   "source": [
    "### Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "572d85a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('train_test_split.npz', allow_pickle=True)\n",
    "x_val = data['x_val']\n",
    "y_val = data['y_val']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "328afd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a45ae26288c47f4b5f4fd45e0742bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Videos:   0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738c24c5a15c4087a26a443a2d924f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fc437bb9d1464bb2593e90fba08f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd951f38a58241be8fcc5d8873433403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd06d411e184021a53b5d3dd2908834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bcdffe447547ebbd04667e65ffc2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888ee8e5c1af40c881b3f30b8fd683f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb26e36c1cd4863ac0a4644495392a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df9847c56aa4671ab9ef5b528ec71c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf62e82c192e4296be2b8420e882b453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eedd0016e3074ea88b4f059627477a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6023d3f69ba24d47975a60219eaca81f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f545030978148458c7e68bd384601f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4d92c5585d4d16ad869896b96a441b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d6d1a3f5b64ae7a14f4e6a002c2457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a57fb4dc7944c788a008638f989ed31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6e415a31354819b8c95e274c75087f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc530de3f6e84788a4cfb75b5f87f080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3bb42d97c44abcb0c71d99847ed992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8f30bd23ab4d78a9cee0b320717eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93a35c19c6f421ba3fa8a84cf4f251c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610a25bf74834740823d1fd1df3c8c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a2f5cc4ad741498dc1d61fff7fa2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8cfa8796b94e53be9235aded1ffa35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4d7a78fa5d41db89ece4371deaf884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8516449bc6974dbabb59049cd3c0e363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825bd07d5df14abcb89f76334f48e385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e832f8b9520046fe9facd546eb693cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b3b54265e04704a56141e408026386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8c650141b24c839afbce0d92b87d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d5d7424e584aa4b876ddfac607a02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae537ad325074425877fdb15fc8720ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4c91eae9c7469596a88d836cd4a834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80edaeb03a64f5f8328c505460d3cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f0437bb85b401ea806c4c20c1bd193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfd70cd518f4dd090d38e8a160f06ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee62b527dd3140ebbd3704709642a51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a3f92db1bd4be6bafa6f0d69570a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda885235b0f4c7e882c2d3991d82bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3a0ff8f8ab4e75a7de751f902f0395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d6366cb80d416dafa682bd9dc6e482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307f5b0403ad4b2fad6036d2200d37a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1364d71eedfb461c9169216f8eeab93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002adb7e997846c8844e36556cccf410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e29a0cc36a640579337343416008bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a104119a01d4bdf9a4a20c23baf3cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c82eef9b194ac58b7bf6a55878e0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce9b16948224a538a45522a1cb91de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f1ec044ebc4a4fb49a3d66b72aa793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0095b79402e64263bb36b64435c0d30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3c3d9e00894efcb9262e32b8a97ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d1550530944e069bc80463b1e1e94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b489353b5941699c88dc4de82bcb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad12ff692c849a08368486492d45eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a5084d256c4e9980f45c0a57961fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e4abc703254d00a9e48fc597028774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b01a33804d4b80aea30d7712ce9074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3482f2d897224e6c95f4168bd5dbbf82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fd67fef72b44d08071deb81ae47888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb7d1f2cf6d413880e49b0a8750a16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3dc4284c8d47eab6dab88256acc485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5376782e2c014926b0e240ad5b60f5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91718acf038a49ef909fc021ed5924f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94186640add4bbbaff98f51b6598569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8781e5f7df4ea485b3922ffe6eddcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ace958775d2493cb6babd1290739cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d229c58905704b9caeb8ae389d98df73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e5e5fc26ad41738735a4f94e1c807b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0459e3f8a89b4e24901406832bbadc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c6350351804b979bb37df39a5b7bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e8e82cbced4d9db24e43cee1289cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92da3890a58c4ecca5a988443ce469ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35819745dd934c2fb300948a5e0e7a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e683be94f84347729796d04ab9523d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e9f7b8db154df2a089b8570f4b256f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0828e1b463c3488598eae595c6d48302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259f59dfe3754245a5ef38dc4e7b26fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2e49a1f2154de1ab781951f2ec5c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db48ad1bdab848c4b47ca6b91b8598f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f83c9e7e734280b6c18df3f6967061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ccc51cd473a427ea4e3c17ea516e9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa87c18f4b54d8fb8d4b3312e09c9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1249bc4554154434a6af69d4b5ca32ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6d4f4eed7141dba40f2b33893fd99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a506e13a075f4b6e918539dd35dbb7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fa5965f0814ae5b61b929e7f4861ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1013cb1b124c81920387bb5cf2b053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c138ea7b5d8d42d590bda018220c6d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5e6b0b642d4b398da64bb0e5a4f33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a482c0eea4446fdba1a1b3c20e99f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1498233824614f2694778ddef9c2d13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2085da206fef4a40844cf1983818223d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1b9de0825c421abb27b51e69e661cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabb714bc98b46e393f557834503b287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455dc79e326e43e4988052c0cefe13d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc035f8bf714e8daa3a65f06edb91c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf8a3fc6e374f0e99e378d70aa0365e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a206004bfdf349cbbe5cd109bce56406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d059269b6c4063989c1b7bb224a518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df899bdbd0e4cbea2880959311f1880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58edd3aa2a3d40d7b18496d51faa48e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef65816452744fd92f0bd67679249e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0f59c0eaf348bdbae803e2eff0ab1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6eb41e6f974d2f82de28f79faa4cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2429609177be444f93f5a571af83315d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd16bdae9948408384f1124c20c08298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44c7c75418b4809b957890097ccdca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7c4827a4a0404297fc0371a8d46c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59535eaa4dee436cab5702027970f21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5597070ec140919b1920047a105e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa4891fddce4c79a2bee4e0e9a5136e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba5ebdfc87f42c99cfd4348550c6b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a33781fb2140eb8c3909c22b0767fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f035d7e291f54948ac0e69bb0ec7750a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed59651e3824af7aa0185f31cedbdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6e93fe9185408d822a8533ee1614ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 24\n",
    "video_paths = x_val\n",
    "video_labels = y_val\n",
    "labels = []\n",
    "val_preds= []\n",
    "probabs = []\n",
    "losses = []\n",
    "\n",
    "model.eval()\n",
    "# classifier.eval()\n",
    "\n",
    "running_loss = 0.0\n",
    "total_batches = 0\n",
    "total_images = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "video_bar = tqdm(zip(video_paths, video_labels),desc='Videos', total=len(video_paths))\n",
    "for path, label in video_bar:\n",
    "    image_files = sorted([ os.path.join(path, f)\n",
    "    for f in os.listdir(path)\n",
    "    if f.lower().endswith('.png')\n",
    "    ])\n",
    "\n",
    "    labels.extend([label] * len(image_files))  # Repeat label for each image\n",
    "    total_images += len(image_files)\n",
    "\n",
    "    for i in tqdm(range(0, len(image_files), batch_size), desc='Image Batches', leave=False):\n",
    "        total_batches += 1\n",
    "        batch_paths = image_files[i:i+batch_size]\n",
    "        images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "        inputs = processor(images=images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        batch_labels = torch.tensor([label] * len(images), dtype=torch.float).unsqueeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            batch_logits = outputs.logits\n",
    "\n",
    "            # running_loss += criterion(batch_logits, batch_labels).item()* batch_labels.size(0)\n",
    "            loss = criterion(batch_logits, batch_labels).detach().cpu().view(-1).tolist()\n",
    "            losses.extend(loss)\n",
    "\n",
    "            prob=torch.sigmoid(batch_logits).detach().cpu().view(-1).tolist()\n",
    "            probabs.extend(prob)\n",
    "\n",
    "            preds = (torch.sigmoid(batch_logits) > 0.5).int()\n",
    "            correct += (preds == batch_labels.int()).sum().item()\n",
    "            total += batch_labels.size(0)\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "# val_loss = running_loss / total_batches if total_batches > 0 else 0\n",
    "val_loss = sum(losses) / total if total > 0 else 0\n",
    "\n",
    "end_time = time.time()\n",
    "vali_time = end_time - start_time\n",
    "vali_inf_time = vali_time / total_images\n",
    "vali_fps = total_images / vali_time\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dadf6964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 85.27%\n",
      "Validation Log Loss: 1.0583\n",
      "Validation Time: 8.06 minutes\n",
      "Validation Single Inference Time: 0.01 seconds\n",
      "Validation FPS: 80.00 images/second\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Validation Log Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(f\"Validation Time: {vali_time/60:.2f} minutes\")\n",
    "print(f\"Validation Single Inference Time: {vali_inf_time:.2f} seconds\")\n",
    "print(f\"Validation FPS: {vali_fps:.2f} images/second\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2827c90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(1.5, 1.2, 'TF')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix([int(x) for x in labels], [1 if p > 0.5 else 0 for p in probabs])\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=[\"Predicted Real\", \"Predicted Fake\"],\n",
    "            yticklabels=[\"Actual Real\", \"Actual Fake\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix (TF, FF, TR, FR)\")\n",
    "\n",
    "# Labels for clarity (optional)\n",
    "plt.text(0.5, 0.2, \"TR\", ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
    "plt.text(1.5, 0.2, \"FF\", ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
    "plt.text(0.5, 1.2, \"FR\", ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
    "plt.text(1.5, 1.2, \"TF\", ha=\"center\", va=\"center\", color=\"black\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6142f79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confident False Real: 2352\n",
      "Confident False Fake: 2903\n"
     ]
    }
   ],
   "source": [
    "CFR=0\n",
    "CFF=0\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if labels[i]==1 :\n",
    "        if probabs[i] < 0.3:\n",
    "            CFR+=1\n",
    "    else:\n",
    "        if probabs[i] > 0.7:\n",
    "            CFF+=1\n",
    "\n",
    "print(f\"Confident False Real: {CFR}\")\n",
    "print(f\"Confident False Fake: {CFF}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1962210",
   "metadata": {},
   "source": [
    "Measuring Gflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f0f5b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:4.5 GMac\n",
      "Module:  Global\n",
      "aten.convolution: 14.75 MMac\n",
      "aten.addmm: 4.17 GMac\n",
      "aten.bmm: 140.14 MMac\n",
      "aten.mm: 173.41 MMac\n",
      "\n",
      "Module:  swin\n",
      "aten.convolution: 14.75 MMac\n",
      "aten.addmm: 4.17 GMac\n",
      "aten.bmm: 140.14 MMac\n",
      "aten.mm: 173.41 MMac\n",
      "\n",
      "Module:  classifier\n",
      "aten.addmm: 769 Mac\n",
      "\n",
      "FLOP: 9.01 GFLOP\n",
      "Params: 14.18 M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Replace this with your actual model instance\n",
    "input_shape = (3, 224, 224)  # Typical for ViT, adjust as needed\n",
    "\n",
    "macs, params = get_model_complexity_info(\n",
    "    model, input_shape, as_strings=False, print_per_layer_stat=True, backend='aten'\n",
    ")\n",
    "\n",
    "print(f\"FLOP: {2*macs/1000000000:.2f} GFLOP\")\n",
    "print(f\"Params: {params/1000000:.2f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1f8c59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 27.52M\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d378a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool1d'>.\n",
      "Params: 27.50M, GFLOPs: 8.73\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "input_tensor = torch.randn(1, 3, 224, 224).to(device)\n",
    "tmacs, tparams = profile(model, inputs=(input_tensor,))\n",
    "print(f\"Params: {tparams / 1e6:.2f}M, GFLOPs: {2*tmacs / 1e9:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151b1519",
   "metadata": {},
   "source": [
    "# Old Codes Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Imports and setup (unchanged unless noted) ===\n",
    "# import os, time, numpy as np, torch\n",
    "# import torch.nn as nn\n",
    "# from PIL import Image\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # ADDED: import AutoModel to access backbone features instead of classification logits\n",
    "# from transformers import AutoImageProcessor, AutoModel  # ADDED [use AutoModel backbone]\n",
    "# # from transformers import AutoModelForImageClassification  # COMMENTED OUT: replaced by AutoModel backbone\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # === Data ===\n",
    "# data = np.load('train_test_split.npz', allow_pickle=True)\n",
    "# x_train = data['x_train']\n",
    "# y_train = data['y_train']\n",
    "\n",
    "# # === Processor ===\n",
    "# # Stays the same to ensure correct resize/normalization for the Swin checkpoint\n",
    "# processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")  # unchanged\n",
    "\n",
    "# # === Model / Backbone ===\n",
    "# # COMMENTED OUT: classification head version (we will use the backbone to get features for LSTM)\n",
    "# # model = AutoModelForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", num_labels=1, ignore_mismatched_sizes=True)  # COMMENTED OUT\n",
    "\n",
    "# # ADDED: load Swin backbone to get hidden states (features) per frame\n",
    "# model = AutoModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")  # ADDED\n",
    "# hidden_size = model.config.hidden_size  # ADDED: feature size for LSTM input\n",
    "\n",
    "# # ADDED: temporal head (BiLSTM + Linear -> 1 logit)\n",
    "# temporal_lstm = nn.LSTM(\n",
    "#     input_size=hidden_size,   # from Swin hidden size\n",
    "#     hidden_size=512,\n",
    "#     num_layers=2,\n",
    "#     batch_first=True,\n",
    "#     bidirectional=True,\n",
    "#     dropout=0.1\n",
    "# ).to(device)  # ADDED\n",
    "\n",
    "# temporal_head = nn.Linear(512 * 2, 1).to(device)  # ADDED\n",
    "\n",
    "# # Keep BCEWithLogitsLoss; we will reduce manually after computing the per-video loss\n",
    "# criterion = nn.BCEWithLogitsLoss(reduction='none')  # unchanged\n",
    "\n",
    "# model = model.to(device)\n",
    "# model.train()\n",
    "# temporal_lstm.train()   # ADDED\n",
    "# temporal_head.train()   # ADDED\n",
    "\n",
    "# # === Freezing / unfreezing ===\n",
    "# # Freeze everything first (backbone only; temporal head stays trainable)\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False  # unchanged (applies to backbone)\n",
    "\n",
    "# # Diagnostic: number of encoder layers in Swin\n",
    "# print(len(model.swin.encoder.layers), \"total layers\")  # unchanged (SwinModel still exposes .swin.encoder.layers)\n",
    "\n",
    "# # Unfreeze last n encoder layers\n",
    "# n = 1\n",
    "# for layer in model.swin.encoder.layers[-n:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "#     display(f'Unfreezing layer: {layer}')  # unchanged\n",
    "\n",
    "# # COMMENTED OUT: there is no classifier anymore in the backbone model\n",
    "# # for param in model.classifier.parameters():\n",
    "# #     param.requires_grad = True  # COMMENTED OUT\n",
    "\n",
    "# # === Optimizer ===\n",
    "# # UPDATED: optimize unfrozen Swin encoder params + temporal head params (LSTM + head)\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     # list(model.classifier.parameters()) +   # COMMENTED OUT\n",
    "#     [p for p in model.swin.encoder.parameters() if p.requires_grad]  # keep unfrozen encoder params\n",
    "#     + list(temporal_lstm.parameters())  # ADDED\n",
    "#     + list(temporal_head.parameters()), # ADDED\n",
    "#     lr=1e-4\n",
    "# )\n",
    "\n",
    "# # === Checkpoint ===\n",
    "# if os.path.exists('swin_tiny_lstm.pth'):\n",
    "#     print(\"Loading existing model weights...\")\n",
    "#     state = torch.load('swin_tiny_lstm.pth', map_location='cpu')\n",
    "#     # UPDATED: handle both old single-module checkpoints and new dict checkpoints\n",
    "#     if isinstance(state, dict) and 'backbone' in state and 'temporal_lstm' in state and 'temporal_head' in state:\n",
    "#         model.load_state_dict(state['backbone'], strict=False)        # UPDATED\n",
    "#         temporal_lstm.load_state_dict(state['temporal_lstm'])         # ADDED\n",
    "#         temporal_head.load_state_dict(state['temporal_head'])         # ADDED\n",
    "#     else:\n",
    "#         # Backward-compat: prior single-module weights\n",
    "#         model.load_state_dict(state, strict=False)                     # UPDATED\n",
    "#     model = model.to(device)\n",
    "#     temporal_lstm = temporal_lstm.to(device)                           # ADDED\n",
    "#     temporal_head = temporal_head.to(device)                           # ADDED\n",
    "#     print(\"Resuming training with loaded model weights.\")\n",
    "# else:\n",
    "#     print(\"Model weights not found, starting training...\")\n",
    "\n",
    "# # === Training hyperparams ===\n",
    "# batch_size = 16\n",
    "# labels = []\n",
    "# video_paths = x_train\n",
    "# video_labels = y_train\n",
    "# total_batches = 0\n",
    "\n",
    "# num_epochs = 20  # unchanged\n",
    "\n",
    "# train_loss = np.zeros(num_epochs)\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# epoch_bar = tqdm(range(num_epochs), desc='Epochs')\n",
    "# for epoch in epoch_bar:\n",
    "#     model.train()\n",
    "#     temporal_lstm.train()   # ADDED\n",
    "#     temporal_head.train()   # ADDED\n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     total_batches = 0\n",
    "\n",
    "#     video_bar = tqdm(zip(video_paths, video_labels), desc='Videos', total=len(video_paths))\n",
    "#     for path, label in video_bar:\n",
    "#         image_files = sorted([\n",
    "#             os.path.join(path, f)\n",
    "#             for f in os.listdir(path)\n",
    "#             if f.lower().endswith('.png')\n",
    "#         ])\n",
    "\n",
    "#         labels.extend([label] * len(image_files))  # unchanged\n",
    "\n",
    "#         video_run_loss = 0.0\n",
    "#         batches = 0\n",
    "\n",
    "#         # ADDED: collect per-frame embeddings for the whole video\n",
    "#         frame_feats = []  # ADDED\n",
    "\n",
    "#         for i in range(0, len(image_files), batch_size):\n",
    "#             batch_paths = image_files[i:i+batch_size]\n",
    "#             images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "#             inputs = processor(images=images, return_tensors=\"pt\")\n",
    "#             inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "#             # COMMENTED OUT: classification logits per frame (we now extract features)\n",
    "#             # outputs = model(**inputs)                               # COMMENTED OUT\n",
    "#             # batch_logits = outputs.logits                           # COMMENTED OUT\n",
    "\n",
    "#             # ADDED: forward through backbone to get last_hidden_state and pool tokens\n",
    "#             with torch.no_grad():  # backbone can stay mostly frozen; gradients not needed here unless fine-tuning\n",
    "#                 outputs = model(**inputs, output_hidden_states=False, return_dict=True)  # ADDED\n",
    "#                 # Swin returns last_hidden_state of shape (B, seq_len, hidden_size); mean-pool over tokens\n",
    "#                 last_hidden = outputs.last_hidden_state  # (B, S, C)  # ADDED\n",
    "#                 pooled = last_hidden.mean(dim=1)         # (B, C)     # ADDED\n",
    "#             frame_feats.append(pooled.detach())          # ADDED\n",
    "\n",
    "#             batches += 1\n",
    "#             total_batches += 1\n",
    "\n",
    "#         # ADDED: assemble sequence and run LSTM head to get a single logit per video\n",
    "#         if len(frame_feats) > 0:\n",
    "#             seq = torch.cat(frame_feats, dim=0)               # (T, C)     # ADDED\n",
    "#             seq = seq.unsqueeze(0)                            # (1, T, C)  # ADDED\n",
    "\n",
    "#             optimizer.zero_grad()                             # ADDED\n",
    "#             lstm_out, _ = temporal_lstm(seq)                  # (1, T, H*2) # ADDED\n",
    "#             last = lstm_out[:, -1, :]                         # (1, H*2)    # ADDED\n",
    "#             video_logit = temporal_head(last)                 # (1, 1)      # ADDED\n",
    "\n",
    "#             video_label = torch.tensor([[label]], dtype=torch.float, device=device)  # (1,1)  # ADDED\n",
    "#             loss = criterion(video_logit, video_label)        # (1,1) with reduction='none'   # ADDED\n",
    "#             loss = loss.mean()                                # scalar for backward            # ADDED\n",
    "#             loss.backward()                                   # ADDED\n",
    "#             optimizer.step()                                  # ADDED\n",
    "\n",
    "#             loss_item = loss.item()                           # ADDED\n",
    "#             running_loss += loss_item                         # ADDED\n",
    "#             video_run_loss += loss_item                       # ADDED\n",
    "\n",
    "#         video_loss = video_run_loss / max(1, batches) if batches > 0 else 0\n",
    "#         video_bar.set_postfix(Last_Loss=f'{video_loss:.4f}')\n",
    "\n",
    "#     avg_loss = running_loss / max(1, total_batches)\n",
    "#     train_loss[epoch] = avg_loss\n",
    "#     epoch_bar.set_postfix(Curr_Loss=f'{avg_loss:.4f}')\n",
    "\n",
    "# end_time = time.time()\n",
    "# training_time = end_time - start_time\n",
    "\n",
    "# # === Save checkpoint ===\n",
    "# # UPDATED: save both backbone and temporal head\n",
    "# torch.save({\n",
    "#     'backbone': model.state_dict(),          # UPDATED\n",
    "#     'temporal_lstm': temporal_lstm.state_dict(),  # ADDED\n",
    "#     'temporal_head': temporal_head.state_dict()   # ADDED\n",
    "# }, 'swin_tiny_lstm.pth')\n",
    "\n",
    "# # === Save training loss (unchanged) ===\n",
    "# if os.path.exists('swin_tiny_train_loss.npy'):\n",
    "#     training_loss = np.load('swin_tiny_train_loss.npy')\n",
    "#     training_loss = np.concatenate((training_loss, train_loss))\n",
    "#     np.save('swin_tiny_train_loss.npy', training_loss)\n",
    "# else:\n",
    "#     training_loss = train_loss\n",
    "#     np.save('swin_tiny_train_loss.npy', training_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 24\n",
    "# video_paths = x_val\n",
    "# video_labels = y_val\n",
    "# labels = []\n",
    "# val_preds= []\n",
    "# probabs = []\n",
    "# losses = []\n",
    "\n",
    "# model.eval()\n",
    "# # classifier.eval()\n",
    "\n",
    "# running_loss = 0.0\n",
    "# total_batches = 0\n",
    "# total_images = 0\n",
    "# correct = 0\n",
    "# total = 0\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# video_bar = tqdm(zip(video_paths, video_labels),desc='Videos', total=len(video_paths))\n",
    "# for path, label in video_bar:\n",
    "#     image_files = sorted([ os.path.join(path, f)\n",
    "#     for f in os.listdir(path)\n",
    "#     if f.lower().endswith('.png')\n",
    "#     ])\n",
    "\n",
    "#     labels.extend([label] * len(image_files))  # Repeat label for each image\n",
    "#     total_images += len(image_files)\n",
    "\n",
    "#     for i in tqdm(range(0, len(image_files), batch_size), desc='Image Batches', leave=False):\n",
    "#         total_batches += 1\n",
    "#         batch_paths = image_files[i:i+batch_size]\n",
    "#         images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "#         inputs = processor(images=images, return_tensors=\"pt\")\n",
    "#         inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "#         batch_labels = torch.tensor([label] * len(images), dtype=torch.float).unsqueeze(1).to(device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "#             batch_logits = outputs.logits\n",
    "\n",
    "#             # running_loss += criterion(batch_logits, batch_labels).item()* batch_labels.size(0)\n",
    "#             loss = criterion(batch_logits, batch_labels).detach().cpu().view(-1).tolist()\n",
    "#             losses.extend(loss)\n",
    "\n",
    "#             prob=torch.sigmoid(batch_logits).detach().cpu().view(-1).tolist()\n",
    "#             probabs.extend(prob)\n",
    "\n",
    "#             preds = (torch.sigmoid(batch_logits) > 0.5).int()\n",
    "#             correct += (preds == batch_labels.int()).sum().item()\n",
    "#             total += batch_labels.size(0)\n",
    "\n",
    "# accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "# # val_loss = running_loss / total_batches if total_batches > 0 else 0\n",
    "# val_loss = sum(losses) / total if total > 0 else 0\n",
    "\n",
    "# end_time = time.time()\n",
    "# vali_time = end_time - start_time\n",
    "# vali_inf_time = vali_time / total_images\n",
    "# vali_fps = total_images / vali_time\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
